---
title: "Modeling"
author: "Alex Kan, Jessica Ho, Katherine Wang, Ishan Shah, Svetak Sundhar"
date: "April 20, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(stringr)
library(data.table)

library(e1071)
library(tree)

library(MASS)
library(ROCR)

library(nnet)

set.seed(1)
```

```{r}
az <- read.csv("data/phoenixAg.csv")
az$businessName <- gsub("\"", "", az$businessName )

temp <- az %>% 
    dplyr::select(-c(businessID, businessName, city, categories))

# Encode average review as 2 if less than 3, or 3 if >= 3 
#temp$averageReviewBusiness <- ifelse(temp$averageReviewBusiness < 3.5, 0, 1)

#training and test sets
train <- sample(1:(0.80*nrow(az)),replace=FALSE) 
az.train <- temp[train,]
az.test <- temp[-train,]

az.train$rate <- ifelse(az.train$averageReviewBusiness < 3.5, 0, 1)
az.test$rate <- ifelse(az.test$averageReviewBusiness < 3.5, 0, 1)
```

# PCA:
```{r, warning = F }

### More feature selection necessary for PCA 
par(mfrow = c(1,2))

pca <- prcomp(temp)

# Sort PCs by highest variance 
bestPCs <- sort(pca$sdev, decreasing = T)[1:2]

plot(pca$x[,], col = temp$averageReviewBusiness, xlab = "PC1", ylab = "PC2", main = "All PCs")
plot(pca$x[pca$sdev == bestPCs, ], col = temp$averageReviewBusiness, xlab = "PC1", ylab = "PC2", main = "Two Highest Variance")

```

# SVM: 
```{r}
#Fit a model. The function syntax is very similar to lm function
model_svm <- svm(rate ~ reviewCountBusiness + isOpen + percAsian + percBlack + 
    percHispanic + percNHW + zipPopulation + zipYouthPopulation + 
    medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + 
    isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast , az.train)
 
#Use the predictions on the data
pred <- predict(model_svm, az.train)
 
#Plot the predictions and the plot to see our model fit
#points(az.train$x, pred, col = "blue", pch=4)
 
#Linear model has a residuals part which we can extract and directly calculate rmse
error <- model_svm$residuals 
lm_error <- sqrt(mean(error^2)) 
 
#For svm, we have to manually calculate the difference between actual values (train$y) with our predictions (pred)
error_2 <- az.train$y - pred
svm_error <- sqrt(mean(error_2^2)) 
 
 
# perform a grid search
svm_tune <- tune(svm,rate ~ reviewCountBusiness + isOpen + percAsian + percBlack + 
    percHispanic + percNHW + zipPopulation + zipYouthPopulation + 
    medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + 
    isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast , data = az.train,
 ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
print(svm_tune)
 
best_mod <- svm_tune$best.model
best_mod_pred <- predict(best_mod, az.train) 
 
error_best_mod <- az.train$y - best_mod_pred 
 
# this value can be different on your computer
# because the tune method randomly shuffles the data
best_mod_RMSE <- sqrt(mean(error_best_mod^2)) 
 
plot(svm_tune)
 
plot(az.train,pch=16)
points(train$x, best_mod_pred, col = "blue", pch=4)

```

# Decision Trees: 
```{r}
tree.mod <- tree(data = temp %>% 
                     dplyr::select(-c(averageReviewBusiness, avgUserPastReview, categories)), avgReviewStars ~.)

#plot(tree(data = temp,
#     as.formula(paste("avgReviewStars", "~",
#                      paste(colnames(temp)[8:16], collapse = "+"),
#                      sep = ""))))

plot(tree.mod)
text(tree.mod, pretty = 0)
```

# Kmeans: 

K means supports our justification for splitting reviews into two categories (<3 and >=3). The "elbow" point on the Error vs. K (# of clusters) plot provides evidence.
```{r}
# euclidianGene <- as.data.frame(cbind(euclidianMeans$cluster, Subtypes))

# Drop start labels and then perform kmeans 

# See if k = 5 clusters best models the 5 different star levels
wss <- sapply(1:5, 
              function(k){kmeans((temp %>% 
                                   dplyr::select(-c(averageReviewBusiness, categories))), k, nstart = 25)$tot.withinss})

plot(1:5, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of Clusters",
     ylab="Total Within-lusters Sum of Squares",
     main="WSS vs. Number of Clusters")


# confusion matrix 
```

# Logistic Regression

```{r}
az.train$rate <- ifelse(az.train$avgReviewStars < 3.5, 0, 1)
az.test$rate <- ifelse(az.test$avgReviewStars < 3.5, 0, 1)

log.mod <- glm(rate~reviewCountBusiness + isOpen + percAsian + percBlack + percHispanic + percNHW + zipPopulation + zipYouthPopulation + medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train, family = binomial)

summary(log.mod)

#confusion matrix
pred <- predict(log.mod, az.test, type = "response")
glm.pred <- rep("0", length(pred))
glm.pred[pred > .5] <- "1"
table(glm.pred, az.test$rate)

mean(glm.pred != az.test$rate)

roc <- prediction(pred, az.test$rate)
rocperf <- performance(roc, 'tpr', 'fpr')
plot(rocperf, colorize = TRUE)


#ROCR Curve
library(ROCR)
ROCRpred <- prediction(predict, dresstrain$Recommended)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2,1.7))
#plot glm
library(ggplot2)
ggplot(dresstrain, aes(x=Rating, y=Recommended)) + geom_point() + 
stat_smooth(method="glm", family="binomial", se=FALSE)
```

# Quadratic Discriminant Analysis

Quadratic Discriminant Analysis allows for non-linear (quadratic) decision boundaries, unlike Linear Discimrinant Analysis. QDA require the number of predictor variables (p) to be less then the sample size (n). We are assuming predictor variables X are drawn from a multivariate Gaussian (aka normal) distribution and that the covariance matrix can be different for each class so we must estimate the covariance matrix separately for each class. However, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix is clearly untenable.

First, we will define a binary variable for averageReviewBusiness. Ratings greater than 3 are considered good ratings and ratings below 3 are bad ratings.

We will create the training and test sets(80%/20%).

Now, we will run QDA and look at the test MSE (mean squared error).
```{r}
#az$rate <- ifelse(az$averageReviewBusiness < 3, 0,1)

#Create Train & Test Sets
#train <- sample(1:(0.80*nrow(az)),replace=FALSE)
#az.train <- temp[train,]
#az.test <- temp[-train,]
```

```{r}
qda.fit <- qda(rate~reviewCountBusiness + isOpen + percAsian + percBlack + percHispanic + percNHW + zipPopulation + zipYouthPopulation + medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train)

qda.fit <- qda(rate ~ reviewCountBusiness + isOpen + percAsian + percBlack + 
    percHispanic + percNHW + zipPopulation + zipYouthPopulation + 
    medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + 
    isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train)

qda.fit

#predict QDA on test set
qda.predict <- predict(qda.fit, newdata=az.test)
qda.class <- qda.predict$class
#Confusion matrix
table(qda.class,az.test$rate)

#Overall fraction of incorrect test predictions (MSE: mean squared error)
mean(qda.class != az.test$rate)
```

```{r, echo=FALSE}
qda.p <- prediction(qda.predict$posterior[,2], az.test$rate) %>%
  performance(measure = "tpr", x.measure = "fpr")

plot(qda.p, col = "red",main="QDA ROC Curve")

#QDA AUC
prediction(qda.predict$posterior[,2], az.test$rate) %>%
  performance(measure = "auc") %>%
  .@y.values
```


Neural Nets

```{r}
#define training set
seedstrain<- az.train
#define test set
seedstest <- az.test

ideal <- class.ind(az$avgreviewstars)
seedsANN <- nnet(az[seedstrain, -44], az[seedstrain, ], size=10, softmax=TRUE)

predict(seedsANN, az[seedstrain,-44], type="rate")
 table(predict(seedsANN, az[seedstest,-44], type="rate"),seeds[seedstest,]$rate)
```


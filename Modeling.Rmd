---
title: "Modeling"
author: "Alex Kan, Jessica Ho, Katherine Wang, Ishan Shah, Svetak Sundhar"
date: "April 20, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(stringr)
library(data.table)

library(e1071)
library(tree)

library(MASS)
library(ROCR)

set.seed(1)
```

```{r}
az <- read.csv("data/phoenixAg.csv")
az$businessName <- gsub("\"", "", az$businessName)

temp <- az %>% 
    dplyr::select(-c(businessID, businessName, city))

# Encode average review as 2 if less than 3, or 3 if >= 3 
temp$averageReviewBusiness <- ifelse(temp$averageReviewBusiness < 3, 2, 3)

#training and test sets
train <- sample(1:(0.80*nrow(az)),replace=FALSE) 
az.train <- temp[train,]
az.test <- temp[-train,]

az.train$rate <- ifelse(az.train$averageReviewBusiness < 3, 0, 1)
az.test$rate <- ifelse(az.test$averageReviewBusiness < 3, 0, 1)
```

PCA:
```{r, warning = F }

### More feature selection necessary for PCA 
par(mfrow = c(1,2))

pca <- prcomp(temp %>% 
                dplyr::select(-categories))

# Sort PCs by highest variance 
bestPCs <- sort(pca$sdev, decreasing = T)[1:2]

plot(pca$x[,], col = temp$averageReviewBusiness, xlab = "PC1", ylab = "PC2", main = "All PCs")
plot(pca$x[pca$sdev == bestPCs, ], col = temp$averageReviewBusiness, xlab = "PC1", ylab = "PC2", main = "Two Highest Variance")

```

SVM: 
```{r}
#svm.mod <- svm(data = temp, avgReviewStars~., kernel = "linear", scale = F, cost = .1)

```

Decision Trees: 
```{r}
tree.mod <- tree(data = temp %>% 
                     dplyr::select(-c(averageReviewBusiness, avgUserPastReview, categories)), avgReviewStars ~.)

#plot(tree(data = temp,
#     as.formula(paste("avgReviewStars", "~",
#                      paste(colnames(temp)[8:16], collapse = "+"),
#                      sep = ""))))

plot(tree.mod)
text(tree.mod, pretty = 0)
```

Kmeans: 

K means supports our justification for splitting reviews into two categories (<3 and >=3). The "elbow" point on the Error vs. K (# of clusters) plot provides evidence.
```{r}
# euclidianGene <- as.data.frame(cbind(euclidianMeans$cluster, Subtypes))

# Drop start labels and then perform kmeans 

# See if k = 5 clusters best models the 5 different star levels
wss <- sapply(1:5, 
              function(k){kmeans((temp %>% 
                                   dplyr::select(-c(averageReviewBusiness, categories))), k, nstart = 25)$tot.withinss})

plot(1:5, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of Clusters",
     ylab="Total Within-lusters Sum of Squares",
     main="WSS vs. Number of Clusters")


# confusion matrix 
```

# Logistic Regression

```{r}
<<<<<<< HEAD
az.train$rate <- ifelse(az.train$avgReviewStars < 3.5, 0, 1)
az.test$rate <- ifelse(az.test$avgReviewStars < 3.5, 0, 1)

log.mod <- glm(rate~reviewCountBusiness + isOpen + percAsian + percBlack + percHispanic + percNHW + zipPopulation + zipYouthPopulation + medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train, family = binomial)
=======
log.mod <- glm(rate~reviewCountBusiness + isOpen + percAsian + percBlack + percHispanic + percNHW + zipPopulation + zipYouthPopulation + medianIncome + percBachelors + avgDaysSinceJoined, data = az.train, family = binomial)
>>>>>>> a77ec3c418c017422d3dc43847fc307da096cb31
summary(log.mod)

#confusion matrix
pred <- predict(log.mod, az.test, type = "response")
glm.pred <- rep("0", length(pred))
glm.pred[pred > .5] <- "1"
table(glm.pred, az.test$rate)

mean(glm.pred != az.test$rate)
```

# Quadratic Discriminant Analysis

Quadratic Discriminant Analysis allows for non-linear (quadratic) decision boundaries, unlike Linear Discimrinant Analysis. QDA require the number of predictor variables (p) to be less then the sample size (n). We are assuming predictor variables X are drawn from a multivariate Gaussian (aka normal) distribution and that the covariance matrix can be different for each class so we must estimate the covariance matrix separately for each class. However, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix is clearly untenable.

First, we will define a binary variable for averageReviewBusiness. Ratings greater than 3 are considered good ratings and ratings below 3 are bad ratings.

We will create the training and test sets(80%/20%).

Now, we will run QDA and look at the test MSE (mean squared error).
```{r}
#az$rate <- ifelse(az$averageReviewBusiness < 3, 0,1)

#Create Train & Test Sets
#train <- sample(1:(0.80*nrow(az)),replace=FALSE)
#az.train <- temp[train,]
#az.test <- temp[-train,]
```

```{r}
<<<<<<< HEAD
qda.fit <- qda(rate~reviewCountBusiness + isOpen + percAsian + percBlack + percHispanic + percNHW + zipPopulation + zipYouthPopulation + medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train)
=======
qda.fit <- qda(rate ~ reviewCountBusiness + isOpen + percAsian + percBlack + 
    percHispanic + percNHW + zipPopulation + zipYouthPopulation + 
    medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + 
    isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train)
>>>>>>> a77ec3c418c017422d3dc43847fc307da096cb31
qda.fit

#predict QDA on test set
qda.predict <- predict(qda.fit, newdata=az.test)
qda.class <- qda.predict$class
#Confusion matrix
table(qda.class,az.test$rate)

#Overall fraction of incorrect test predictions (MSE: mean squared error)
mean(qda.class != az.test$rate)
```

```{r, echo=FALSE}
qda.p <- prediction(qda.predict$posterior[,2], az.test$averageReviewBusiness) %>%
  performance(measure = "tpr", x.measure = "fpr")

plot(qda.p, col = "red",main="QDA ROC Curve")

#QDA AUC
prediction(qda.predict$posterior[,2], az.test$averageReviewBusiness) %>%
  performance(measure = "auc") %>%
  .@y.values
```


---
title: "Modeling"
author: "Alex Kan, Jessica Ho, Katherine Wang, Ishan Shah, Svetak Sundhar"
date: "April 20, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(stringr)
library(data.table)

library(e1071)
library(tree)

library(MASS)
library(ROCR)

library(randomForest)
library(nnet)

library(tfestimators)
library(reticulate)

library(mlr)

set.seed(1)

```

```{r}
az <- read.csv("data/phoenixAg.csv")
az$businessName <- gsub("\"", "", az$businessName )

# Encode average review as 2 if less than 3, or 3 if >= 3 
#temp$averageReviewBusiness <- ifelse(temp$averageReviewBusiness < 3.5, 0, 1)

#training and test sets
train <- sample(1:(0.80*nrow(az)),replace=FALSE) 
az.train <- az[train,]
az.test <- az[-train,]

az.train$rate <- ifelse(az.train$avgReviewStars <= 3.5, 0, 1)
az.test$rate <- ifelse(az.test$avgReviewStars <= 3.5, 0, 1)

mod <- rate ~ reviewCountBusiness + isOpen + percAsian + percBlack + 
    percHispanic + percNHW + zipPopulation + zipYouthPopulation + 
    medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + 
    isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast 

az.train$rate <- as.factor(az.train$rate)
az.test$rate <- as.factor(az.test$rate)
```


# Decision Trees: 
```{r}
tree.mod <- tree(data = az.train, 
                   rate ~ reviewCountBusiness + isOpen + percAsian + percBlack + percHispanic + percNHW + zipPopulation +
                   zipYouthPopulation + medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + 
                   isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast)

#plot(tree(data = temp,
#     as.formula(paste("avgReviewStars", "~",
#                      paste(colnames(temp)[8:16], collapse = "+"),
#                      sep = ""))))

plot(tree.mod)
text(tree.mod, pretty = 0)
```


Random Forest 
```{r}

```


# Kmeans: 

K means supports our justification for splitting reviews into two categories (<3 and >=3). The "elbow" point on the Error vs. K (# of clusters) plot provides evidence.
```{r}
# See if k = 5 clusters best models the 5 different star levels
wss <- sapply(1:5, 
              function(k){kmeans((temp %>% 
                                   dplyr::select(-c(averageReviewBusiness))), k, nstart = 25)$tot.withinss})

plot(1:5, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of Clusters",
     ylab="Total Within-lusters Sum of Squares",
     main="WSS vs. Number of Clusters")


# confusion matrix 
```

# Logistic Regression

```{r}
log.mod <- glm(mod, data = az.train, family = binomial)

summary(log.mod)

# confusion matrix

pred <- predict(log.mod, az.test, type = "response")
glm.pred <- rep("0", length(pred))
glm.pred[pred > .5] <- "1"
table(glm.pred, az.test$rate)

mean(glm.pred != az.test$rate)
```

# Quadratic Discriminant Analysis

Quadratic Discriminant Analysis allows for non-linear (quadratic) decision boundaries, unlike Linear Discimrinant Analysis. QDA require the number of predictor variables (p) to be less then the sample size (n). We are assuming predictor variables X are drawn from a multivariate Gaussian (aka normal) distribution and that the covariance matrix can be different for each class so we must estimate the covariance matrix separately for each class. However, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix is clearly untenable.

First, we will define a binary variable for averageReviewBusiness. Ratings greater than 3 are considered good ratings and ratings below 3 are bad ratings.

We will create the training and test sets(80%/20%).

Now, we will run QDA and look at the test MSE (mean squared error).
```{r}
#az$rate <- ifelse(az$averageReviewBusiness < 3, 0,1)

#Create Train & Test Sets
#train <- sample(1:(0.80*nrow(az)),replace=FALSE)
#az.train <- temp[train,]
#az.test <- temp[-train,]
```

```{r}


qda.fit <- qda(mod, data = az.train)


qda.fit <- qda(rate ~ reviewCountBusiness + isOpen + percAsian + percBlack + 
    percHispanic + percNHW + zipPopulation + zipYouthPopulation + 
    medianIncome + percBachelors + avgDaysSinceJoined + isFastFood + 
    isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train)

qda.fit

#predict QDA on test set
qda.predict <- predict(qda.fit, newdata=az.test)
qda.class <- qda.predict$class
#Confusion matrix
table(qda.class,az.test$rate)

#Overall fraction of incorrect test predictions (MSE: mean squared error)
mean(qda.class != az.test$rate)

```

```{r, echo=FALSE}
qda.p <- prediction(qda.predict$posterior[,2], az.test$rate) %>%
  performance(measure = "tpr", x.measure = "fpr")

plot(qda.p, colorize = T, main="QDA ROC Curve")

#QDA AUC
prediction(qda.predict$posterior[,2], az.test$rate) %>%
  performance(measure = "auc") %>%
  .@y.values
```

```{r}
#ROC Curves for LogReg & QDA

roc <- prediction(pred, az.test$rate)
rocperf <- performance(roc, 'tpr', 'fpr')

plot(rocperf, colorize = T)
plot(qda.p, add = T, colorize = F)

plot(rocperf, colorize = T, main = "LogReg ROC Curve")


```

# Neural Nets

```{r}
#define training set
seedstrain<- az.train
#define test set
seedstest <- az.test

ideal <- class.ind(az$rate)

response <- as.factor(seedstrain$rate)
predictors <- seedstrain %>% dplyr::select(-rate)

seedsANN <- nnet(predictors, response, size=10, softmax = T, linout= T)

predict(seedsANN, az[seedstrain,-44], type="rate")
 table(predict(seedsANN, az[seedstest,-44], type="rate"),seeds[seedstest,]$rate)
```


# Naive Bayes
```{r}
#require(e1071) Holds the Naive Bayes Classifier
head(az.train)
head(az.test)
#Make sure the target variable is of a two-class classification problem only

levels(az.train$rate)
levels(az.test$rate)


bayes.mod <- naiveBayes(rate ~ reviewCountBusiness + isOpen + percAsian + percBlack 
                        + percHispanic + percNHW + zipPopulation + zipYouthPopulation 
                        + medianIncome + percBachelors + avgDaysSinceJoined + 
                          isFastFood + isItalian + isHispanic + isAsian + isAmerican + isBar + isBreakfast, data = az.train)
class(bayes.mod) 

pred <- predict(bayes.mod,az.test$rate)

table(pred, az.test$rate)

mean(pred!=az.test$rate)

```

```{r}
#Naive Bayes ROC Curve
pvec <- ifelse(pred==1,1,0)
vvec <- ifelse(az.test$rate==1,1,0)
mpred <- prediction(pvec, vvec)
perf <- performance(mpred, "tpr", "fpr")
plot(perf, main = "ROC curve for Naive Bayes Classifier",col = "blue", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)

#AUC
perf.auc <- performance(mpred, measure = "auc")
unlist(perf.auc@y.values)
```


```{r}

az <- az %>% 
  dplyr::select(-c(businessID, categories, city))

response <- function() "avgReviewStars"
features <- function() setdiff(names(az), response())

# split into train, test datasets
partitions <- modelr::resample_partition(az, c(test = 0.2, train = 0.8))
az_train <- as.data.frame(partitions$train)
az_test  <- as.data.frame(partitions$test)

# construct feature columns
feature_columns <- feature_columns(
  column_numeric(features())
)

# construct classifier
classifier <- dnn_classifier(
  feature_columns = feature_columns,
  hidden_units = c(10, 20, 10),
  n_classes = 3
)

# construct input function 
iris_input_fn <- function(data) {
  input_fn(data, features = features(), response = response())
}

# train classifier with training dataset
train(classifier, input_fn = iris_input_fn(az_train))

# valuate with test dataset
predictions <- predict(classifier, input_fn = iris_input_fn(az_test))
evaluation <- evaluate(classifier, input_fn = iris_input_fn(az_test))
```


```{r}

az.train <- az.train %>% 
  dplyr::select(-c(businessID, categories, city, businessName))


az.test <- az.test %>% 
  dplyr::select(-c(businessID, categories, city, businessName))
#Creating data from table
#repeating_sequence <- rep.int(seq_len(nrow(az.train)), az.train$Freq) #This will repeat each combination equal to the frequency of each combination
 
#Create the dataset by row repetition created
#az.train <- az.train[repeating_sequence,]
#We no longer need the frequency, drop the feature
#az.train$Freq <- NULL
 
#Fitting the Naive Bayes model
Naive_Bayes_Model <- naiveBayes(mod, data=az.train)
#What does the model say? Print the model summary
Naive_Bayes_Model
 
#Prediction on the dataset
NB_Predictions <- predict(Naive_Bayes_Model, az.test$rate)
NB_Predictions
#Confusion matrix to check accuracy
table(NB_Predictions, az.test$rate)
 
#Getting started with Naive Bayes in mlr

 
#Create a classification task for learning on Titanic Dataset and specify the target feature
task <- makeClassifTask(data = az.train, target = "rate")
 
#Initialize the Naive Bayes classifier
selected_model <- makeLearner("classif.naiveBayes")
 
#Train the model
NB_mlr <- train(selected_model, task)
 
#Read the model learned  
NB_mlr$learner.model
 
#Predict on the dataset without passing the target feature
predictions_mlr <- as.data.frame(predict(NB_mlr, newdata = az.train[,1:3]))
 
##Confusion matrix to check accuracy
table(predictions_mlr[,1],az.train$Survived)
```

```{r}
require(e1071) #Holds the Naive Bayes Classifier

#Make sure the target variable is of a two-class classification problem only


levels(az.train$rate)

model <- naiveBayes(mod, data = az.train)
class(model) 
pred <- predict(model,az.test$rate)
table(pred)

mean(pred != az.test$rate)
```

